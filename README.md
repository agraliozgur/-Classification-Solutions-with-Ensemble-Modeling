
# Classification Solutions with Mechine Learning Ensemble Modeling

By participating in the Titanic competition in Kaggle, an emsemble model was developed for models that gave the best results from multiple machine learning models on a classification problem.



## Dataset 

The sinking of Titanic is one of the most notorious shipwrecks in the history. In 1912, during her voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew.

#### Variables Description¶
1. PassengerId: unique id number to each passenger
2. Survived: passenger survive(1) or died(0)
3. Pclass: passenger class
4. Name: name
5. Sex: gender of passenger
6. Age: age of passenger
7. SibSp: number of siblings/spouses
8. Parch: number of parents/children
9. Ticket: ticket number
10. Fare: amount of money spent on ticket
11. Cabin: cabin category
12. Embarked: port where passenger embarked (C = Cherbourg, Q = Queenstown, S = Southampton)


## Another important intervening Transactions

- Feature Engineering/Selection

- Missing Value removal

- Outlier Detection


## Apply to ML classification model and -- Grid Search -- Cross Validation¶
I will compare 10 machine learning classifier and evaluate mean accuracy of each of them by stratified cross validation.

- Decision Tree
- SVM
- Random Forest
- KNN
- Logistic Regression
- GradientBoostingClassifier
- MLPClassifier
- XGBRFClassifier
- LGBMClassifier
- CatBoostClassifier



## An image of the classification achievements of the models

![githubb1](https://user-images.githubusercontent.com/28312075/117070742-d8ad3000-ad36-11eb-8acc-57f06ac27726.PNG)
  
